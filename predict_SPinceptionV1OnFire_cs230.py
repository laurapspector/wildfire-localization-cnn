'''
May 2019
This script generates baseline metrics for the wildfire localization project. It takes as input
a directory of isolated superpixels and .npy files listing the filenames and ground truth labels,
generated by the script isolate_superpixels.py. It classifies each superpixel using the
model built in inceptionV1OnWildfire_cs230.py, computes the F1 score for prediction, then merges positively
predicted superpixels to compute the IoU compared to the ground truth.
'''
################################################################################

import cv2
import os
import sys
import math
import numpy as np
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels

################################################################################

import tflearn
from tflearn.layers.core import input_data, dropout, fully_connected, activation
from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d
from tflearn.layers.normalization import local_response_normalization
from tflearn.layers.merge_ops import merge
from tflearn.layers.estimator import regression

################################################################################

from inceptionV1OnWildfire_cs230 import construct_inceptionv1onwildfire

###################### USEFUL FUNCTIONS ###########################################

def merge_and_iou(y, y_hat, filenames):
    '''
    Script to compute F1 score and IoU

    Inputs:
        y = numpy array, ground truth labels (0 or 1)
        y_hat = numpy array, predicted labels (0 or 1)
        filenames = numpy array, filenames input to classifier

    Returns:
        F1_score = float, F1 score
        similarity = float, average IoU score
    '''

    # Retain only positive (ground truth or predicted) superpixels
    gt_sp = filenames[np.nonzero(y)]
    pred_sp = filenames[np.nonzero(y_hat)]

    # Create directory to store merged positive superpixels for manual examination
    meta_img = os.path.join(args.data_dir, 'IoU_images')
    # os.mkdir(meta_img)

    # Initialize list to store IoU scores for each image
    similarity = []

    # Remove suffix from filenames in order to reduce to whole image numbers
    # By ignoring superpixel index
    filenames_split = [i.split('_rgb')[0] for i in filenames]
    ordered_test_imgs = []
    # Loop through filenames by whole image
    for img_num in tqdm(set(filenames_split)):
        ordered_test_imgs.append(img_num)
        # Merge all positive superpixels for that image
        gt_merged = merge_superpixels(img_num, gt_sp, meta_img, name='gt')
        pred_merged = merge_superpixels(img_num, pred_sp, meta_img, name='pred')
        
        # Compute IoU
        intersection = cv2.bitwise_and(gt_merged, pred_merged)
        union = cv2.bitwise_or(gt_merged, pred_merged)
        total_intersection = np.sum(np.sum((intersection > 0)))
        total_union = np.sum(np.sum((union > 0)))
        similarity.append(total_intersection / total_union)

    # Save IoU for each image in a numpy array
    np.save(os.path.join(args.output_dir, 'IoU_test'), np.asarray(similarity))
    np.save(os.path.join(args.output_dir, 'ordered_test_imgs_test'), np.asarray(ordered_test_imgs))
    
    # Compute F1 score
    tp = len(np.where(y * y_hat == 1)[0])
    fp = len(np.where((1-y) * y_hat == 1)[0])
    fn = len(np.where((1-y_hat) * y == 1)[0])
    tn = len(y) - tp - fp - fn
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    F1_score = 2 * (precision * recall)/(precision + recall)
    accuracy = (tp + tn) / len(y)

    return round(F1_score, 3), round(np.mean(np.asarray(similarity)), 3), round(accuracy, 3)

def merge_superpixels(img_num, pixel_set, store_dir, name=None):
    '''
    Function to merge positive superpixels (predicted or ground truth)
    for a given image

    Inputs:
        pixel_set = numpy array, filenames corresponding to positive superpixels

    Returns:
        merged = numpy array, the 1 layer image of merged positive superpixels
    '''
    # Generate 'empty' image
    merged = np.zeros(shape=(224, 224, 3), dtype = "uint8")
    merged = merged[:,:,0]

    # Pare down superpixel list to contain only positive superpixels (ground truth or predicted)
    fire_superpixels = [gt_pixel.split('_rgb')[0] + '_gt' + gt_pixel.split('_rgb')[1] for gt_pixel in pixel_set if gt_pixel.split('_rgb')[0].startswith(img_num)]

    # Merge positive superpixels with empty image iteratively
    for superpixel in fire_superpixels:
        superpixel = cv2.imread(superpixel)[:,:,0]
        merged = cv2.bitwise_or(merged, superpixel)

    # Save merged image
    save_path = os.path.join(store_dir, img_num.split('/')[-1] + '_' + name + '_merged.png')
    cv2.imwrite(save_path, merged)

    return merged

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=True,
                          title=None,
                          cmap=plt.cm.Blues):
    '''
    Script to generate a confusion matrix. Taken from sklearn examples.
    '''
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Only use the labels that appear in the data
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


################################################################################

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', default='all_db_fire', help="Location of whole images")
    parser.add_argument('--output_dir', default='all_db_fire/isolated_superpixels', help="Location of X and Y arrays and isolated superpixels")

    args = parser.parse_args()

    assert os.path.isdir(args.output_dir), "Couldn't find the directory {}".format(args.output_dir)

    ################################################################################
    # construct and display model
    # important: make sure you have changed restore=True for the fully connected layer in the model
    model = construct_inceptionv1onwildfire (224, 224, training=False)
    print("Constructed SP-InceptionV1-OnWildfire ...")

    # Load trained weights (here loading Adam optimized model weights)
    model.load(os.path.join("models/SP-InceptionV1-OnWildfire", "inceptiononv1onwildfire_adam-16250"),weights_only=True)

    print("Loaded CNN network weights ...")
    ################################################################################
    
    # load test data
    print("Loading test data...")

    # Load file list
    filenames = np.load(os.path.join(args.output_dir, 'x_test.npy'))

    # load y_test that was saved when running isolate_superpixels_cs230.py as ground truth
    # remember that 'fire' will be 0's
    y_test = np.load(os.path.join(args.data_dir, 'y_test.npy')) 
    y_test = 1 - y_test

    # Run prediction, remember that prediction output is one-hot encoded
    # if column 0 (fire) is ~1., model predicted 'fire'
    print("Predicting...")
   
    # Initialize list to store y_hat labels
    predictions = []
    # Loop through files and predict using loaded model
    for file in tqdm(filenames):
    	img = cv2.imread(file)
    	output = model.predict([img])
    	output = int(round(output[0][0]))
    	predictions.append(output)

    y_pred = np.array(predictions)

    # Compute IoU and F1 score
    print("Computing metrics...")
    (F1_score, similarity, accuracy) = merge_and_iou(y_test, y_pred, filenames)

    print('Accuracy', accuracy)
    print('F1_score', F1_score)
    print('IoU', similarity)

    # Plot normalized confusion matrix
    np.set_printoptions(precision=2)
    class_names = ['non-fire', 'fire']
    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
                          title='Normalized confusion matrix')

    plt.show()
